{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error at 16\n",
      "error at 18\n",
      "error at 20\n"
     ]
    },
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n\tGetHandleVerifier [0x00007FF6766FB5D2+29090]\n\t(No symbol) [0x00007FF67666E689]\n\t(No symbol) [0x00007FF67652B1CA]\n\t(No symbol) [0x00007FF67657EFD7]\n\t(No symbol) [0x00007FF67657F22C]\n\t(No symbol) [0x00007FF6765727CC]\n\t(No symbol) [0x00007FF6765A672F]\n\t(No symbol) [0x00007FF6765726A6]\n\t(No symbol) [0x00007FF6765A6900]\n\t(No symbol) [0x00007FF6765C65D9]\n\t(No symbol) [0x00007FF6765A6493]\n\t(No symbol) [0x00007FF6765709B1]\n\t(No symbol) [0x00007FF676571B11]\n\tGetHandleVerifier [0x00007FF676A18C5D+3295277]\n\tGetHandleVerifier [0x00007FF676A64843+3605523]\n\tGetHandleVerifier [0x00007FF676A5A707+3564247]\n\tGetHandleVerifier [0x00007FF6767B6EB6+797318]\n\t(No symbol) [0x00007FF67667980F]\n\t(No symbol) [0x00007FF6766753F4]\n\t(No symbol) [0x00007FF676675580]\n\t(No symbol) [0x00007FF676664A1F]\n\tBaseThreadInitThunk [0x00007FFEF598257D+29]\n\tRtlUserThreadStart [0x00007FFEF6C4AF28+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 67\u001b[0m\n\u001b[0;32m     62\u001b[0m                     count\u001b[38;5;241m=\u001b[39mcount\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     65\u001b[0m     teardown(driver)\n\u001b[1;32m---> 67\u001b[0m \u001b[43mscraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[85], line 31\u001b[0m, in \u001b[0;36mscraper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     next_page_container\u001b[38;5;241m=\u001b[39mdriver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpagination-div\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m     next_page\u001b[38;5;241m=\u001b[39m\u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_page_container\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCSS_SELECTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.pagination-div:last-child\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(next_page\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouterHTML\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     33\u001b[0m     next_page\u001b[38;5;241m.\u001b[39mclick()            \n",
      "File \u001b[1;32mc:\\Users\\nzamk\\Desktop\\Web scraper\\venv\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:105\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n\tGetHandleVerifier [0x00007FF6766FB5D2+29090]\n\t(No symbol) [0x00007FF67666E689]\n\t(No symbol) [0x00007FF67652B1CA]\n\t(No symbol) [0x00007FF67657EFD7]\n\t(No symbol) [0x00007FF67657F22C]\n\t(No symbol) [0x00007FF6765727CC]\n\t(No symbol) [0x00007FF6765A672F]\n\t(No symbol) [0x00007FF6765726A6]\n\t(No symbol) [0x00007FF6765A6900]\n\t(No symbol) [0x00007FF6765C65D9]\n\t(No symbol) [0x00007FF6765A6493]\n\t(No symbol) [0x00007FF6765709B1]\n\t(No symbol) [0x00007FF676571B11]\n\tGetHandleVerifier [0x00007FF676A18C5D+3295277]\n\tGetHandleVerifier [0x00007FF676A64843+3605523]\n\tGetHandleVerifier [0x00007FF676A5A707+3564247]\n\tGetHandleVerifier [0x00007FF6767B6EB6+797318]\n\t(No symbol) [0x00007FF67667980F]\n\t(No symbol) [0x00007FF6766753F4]\n\t(No symbol) [0x00007FF676675580]\n\t(No symbol) [0x00007FF676664A1F]\n\tBaseThreadInitThunk [0x00007FFEF598257D+29]\n\tRtlUserThreadStart [0x00007FFEF6C4AF28+40]\n"
     ]
    }
   ],
   "source": [
    "def setup():\n",
    "    driver = webdriver.Chrome()\n",
    "    page_url=\"https://community.shopify.com/c/shopify-apps/bd-p/shopify-apps\"\n",
    "    driver.get(page_url)\n",
    "    return driver\n",
    "\n",
    "def teardown(driver):\n",
    "    driver.quit()\n",
    "\n",
    "def save_data_to_file(data, filename):\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    \n",
    "    with open(f'data/{filename}', 'w', encoding='utf-8') as file:\n",
    "        file.write(data)\n",
    "\n",
    "\n",
    "url=[]\n",
    "\n",
    "def scraper():    \n",
    "    driver = setup()\n",
    "    driver.implicitly_wait(0.5)\n",
    "    count=1\n",
    "    \n",
    "    for page_no in range(1,7):\n",
    "        if(page_no==1):\n",
    "            data=driver.find_element(By.CLASS_NAME,\"main-body\").find_elements(By.CSS_SELECTOR,\"div.res-data\")\n",
    "        else:\n",
    "            next_page_container=driver.find_element(By.CLASS_NAME,\"pagination-div\")\n",
    "            \n",
    "            next_page=WebDriverWait(next_page_container, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR,\".pagination-div:last-child\")))\n",
    "            print(next_page.get_attribute(\"outerHTML\"))\n",
    "            next_page.click()            \n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR,\".main-body\")))\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div.res-data\")))\n",
    "            data=driver.find_elements(By.CSS_SELECTOR,\"div.res-data\")\n",
    "            \n",
    "            \n",
    "        for i,eles in enumerate(data):\n",
    "            try:\n",
    "                data = driver.find_elements(By.CSS_SELECTOR, \"div.res-data\")\n",
    "                divs=data[i]\n",
    "                link=divs.find_element(By.CSS_SELECTOR,\"div.res-subject > a\")\n",
    "                post_url=link.get_attribute(\"href\")\n",
    "                url.append(post_url)\n",
    "                \n",
    "                link.click()\n",
    "                element=WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.lia-quilt-row-main\")))\n",
    "                data=element.get_attribute(\"outerHTML\")\n",
    "                save_data_to_file(data, f'page_{count}.html')\n",
    "                count=count+1\n",
    "                # driver.execute_script(\"window.history.go(-1)\")\n",
    "                driver.back()\n",
    "                WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, \"res-data\")))\n",
    "                \n",
    "            except NoSuchElementException as e:\n",
    "                    print(f\"Element not found in div {i+1}: {e}\")\n",
    "            except TimeoutException as e:\n",
    "                    print(f\"Timeout occurred for div {i+1}: {e}\")\n",
    "            except Exception as e:\n",
    "                    print(\"error at\",i+1)\n",
    "                    count=count+1\n",
    "\n",
    "                \n",
    "    teardown(driver)\n",
    "    \n",
    "scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 73\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplies\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(reply_list)\n\u001b[1;32m---> 73\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[0;32m     74\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n",
      "File \u001b[1;32mc:\\Users\\nzamk\\Desktop\\Web scraper\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\nzamk\\Desktop\\Web scraper\\venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nzamk\\Desktop\\Web scraper\\venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\nzamk\\Desktop\\Web scraper\\venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "d={\"title\":[],\"user_name\":[],\"user_title\":[],\"url\":url,\"user_profile_link\":[],\"number_of_posts_by_user\":[],\"number_of_solutions_by_user\":[],\"number_of_kudos_by_user\":[],\"timestamp\":[],\"post_content\":[],\"solved\":[],\"labels_list\":[],\"number_of_views\":[],\"number_of_likes\":[],\"replies\":[]}\n",
    "\n",
    "for file in os.listdir(\"data\"):\n",
    "    with open(f\"data/{file}\",encoding=\"utf8\") as f:\n",
    "        html_doc=f.read()\n",
    "    soup=BeautifulSoup(html_doc,\"html.parser\")    \n",
    "    \n",
    "    title=soup.find(\"h1\",class_=\"page-title\")\n",
    "    d[\"title\"].append(title.text)\n",
    "    \n",
    "    user_name=soup.find(\"a\",class_=\"lia-user-name-link\")\n",
    "    d[\"user_name\"].append(user_name.text)\n",
    "    \n",
    "    user_profile_link=user_name.get_attribute_list(\"href\")\n",
    "    d[\"user_profile_link\"].append(user_profile_link[0])\n",
    "   \n",
    "    user_title_container=soup.find(\"span\",class_=\"lia-message-unread\")\n",
    "    user_title=user_title_container.find_next(\"div\")    \n",
    "    d[\"user_title\"].append(user_title.text.strip())\n",
    "    \n",
    "    number_of_posts_by_user=soup.find(class_=\"lia-message-author-posts-count\")\n",
    "    d[\"number_of_posts_by_user\"].append(number_of_posts_by_user.text)\n",
    "    \n",
    "    number_of_solutions_by_user=soup.find(class_=\"lia-message-author-solutions-count\")\n",
    "    d[\"number_of_solutions_by_user\"].append(number_of_solutions_by_user.text.strip())\n",
    "    \n",
    "    nunber_of_kudos_by_user=soup.find(class_=\"lia-message-author-kudos-count\")\n",
    "    d[\"number_of_kudos_by_user\"].append(nunber_of_kudos_by_user.text.strip())\n",
    "    \n",
    "    publish_date=soup.select(\".DateTime > meta\")[0].get_attribute_list(\"content\")\n",
    "    d[\"timestamp\"].append(publish_date[0])\n",
    "    \n",
    "    post_content_data=soup.find(class_=\"lia-message-body-content\")\n",
    "    for content in post_content_data(['style', 'script']):\n",
    "        content.decompose().strip()\n",
    "    post_content=' '.join(post_content_data.stripped_strings)\n",
    "    d[\"post_content\"].append(post_content)    \n",
    "    \n",
    "    if soup.find(class_=\"lia-panel-feedback-banner-safe\"):\n",
    "       solve=\"Y\"\n",
    "    else:\n",
    "       solve=\"N\"    \n",
    "    d[\"solved\"].append(solve)\n",
    "    \n",
    "    label_data=soup.find_all(class_=\"label\")\n",
    "    labels=[]\n",
    "    for label_arr in label_data:\n",
    "        labels.append((label_arr.text).strip())\n",
    "    d[\"labels_list\"].append(labels)\n",
    "        \n",
    "    view_count=soup.find(class_=\"lia-message-stats-count\")\n",
    "    d[\"number_of_views\"].append(view_count.text.strip())    \n",
    "    \n",
    "    like_count=soup.find(class_=\"MessageKudosCount\")\n",
    "    d[\"number_of_likes\"].append(like_count.text.strip())  \n",
    "    \n",
    "    replies=soup.find(class_=\"threaded-detail-message-list\")\n",
    "    replies=soup.find(class_=\"threaded-detail-message-list\")\n",
    "    replies=soup.find(class_=\"lia-threaded-detail-display-message-view\")\n",
    "    reply_list=[]\n",
    "    if replies:\n",
    "        reply_data=soup.find_all(class_=\"lia-message-body-content\")\n",
    "        for content in reply_data[1](['style', 'script']):\n",
    "            content.decompose().strip()\n",
    "        reply_content=' '.join(reply_data[1].stripped_strings)\n",
    "        reply_list.append(reply_content)\n",
    "    else:\n",
    "        pass\n",
    "    d[\"replies\"].append(reply_list)\n",
    "\n",
    "  \n",
    "    \n",
    "df=pd.DataFrame(data=d);\n",
    "df.to_csv(\"data.csv\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
