{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    return driver\n",
    "\n",
    "def teardown(driver):\n",
    "    driver.quit()\n",
    "\n",
    "def save_data_to_file(data, filename):\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    \n",
    "    with open(f'data/{filename}', 'w', encoding='utf-8') as file:\n",
    "        file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Discussions_urls=[]\n",
    "def  get_discussing_urls():\n",
    "    driver=setup(\"https://community.shopify.com/c/shopify-discussion/ct-p/shopify-discussion\")\n",
    "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.more-boards\")))\n",
    "    load_all=driver.find_elements(By.CSS_SELECTOR,\"button.more-boards\")\n",
    "    \n",
    "    while True:\n",
    "       if len(load_all)>0:\n",
    "           load_all[0].click()\n",
    "           try:\n",
    "                WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.more-boards\")))\n",
    "                load_all=driver.find_elements(By.CSS_SELECTOR,\"button.more-boards\")\n",
    "           except:\n",
    "               load_all=[]\n",
    "       else:\n",
    "           break\n",
    "    \n",
    "    links_list=driver.find_elements(By.CSS_SELECTOR,\".tiled-navigation-boards>a\")\n",
    "    for i,url in enumerate(links_list[:-1]):\n",
    "        \n",
    "        Discussions_urls.append(url.get_attribute(\"href\"))\n",
    "\n",
    "   \n",
    "        \n",
    "get_discussing_urls()\n",
    "\n",
    "df=pd.DataFrame(data={\"Discussions_urls\":Discussions_urls});\n",
    "df.to_csv(\"Discussions_urls_data.csv\")  \n",
    "\n",
    "# print(discussing_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_url=[]\n",
    "def scraper(url):\n",
    "    \n",
    "    driver = setup(url)\n",
    "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"div.pagination-div>a\")))\n",
    "    page_counts = driver.find_elements(By.CSS_SELECTOR,\"div.pagination-div>a\")\n",
    "    last_page=page_counts[-2].text #to get count of number of pages\n",
    "\n",
    "    \n",
    "    for page_no in range(1, last_page):\n",
    "        try:\n",
    "            if page_no == 1:\n",
    "                data = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.res-data\")))\n",
    "            else:\n",
    "                WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"div.pagination-div>a\")))\n",
    "                pagination = driver.find_elements(By.CSS_SELECTOR,\"div.pagination-div>a\")            \n",
    "                driver.implicitly_wait(5)\n",
    "                pagination[-1].click()\n",
    "                data = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.res-data\")))\n",
    "        except Exception as e:\n",
    "            print(\"Out of boundary index. Your all url's are fetched. Wait for execution\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        for i, ele in enumerate(data):\n",
    "            try:\n",
    "                data = driver.find_elements(By.CSS_SELECTOR, \"div.res-data\")\n",
    "                divs=data[i]                \n",
    "                link=divs.find_element(By.CSS_SELECTOR,\"div.res-subject > a\")\n",
    "                post_url = link.get_attribute(\"href\")\n",
    "                single_url.append(post_url)            \n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"some error occur. But url's are fetched.\")\n",
    "    \n",
    "    # print(len(single_url))    \n",
    "    teardown(driver)\n",
    "    \n",
    "    \n",
    "df = pd.read_csv('discussing_urls_data.csv')\n",
    "Discussions_urls_from_csv = df[\"discussing_urls\"].tolist()\n",
    "\n",
    "for i,link in enumerate(Discussions_urls_from_csv): \n",
    "    scraper(link)\n",
    "\n",
    "\n",
    "df=pd.DataFrame(data={\"url\":single_url});\n",
    "df.to_csv(\"post_urls.csv\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('post_urls.csv')\n",
    "urls = df[\"url\"].tolist()\n",
    "\n",
    "def data_pages():\n",
    "    for i,link in enumerate(urls):\n",
    "        # change number acc. to where you pause the last execution. Put last printed number in \"if\" condition to resume.\n",
    "        # if i<=2105: \n",
    "        #     continue;\n",
    "        # else:\n",
    "            driver = setup(link)\n",
    "            element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.lia-quilt-row-main\")))\n",
    "            post_data = element.get_attribute(\"outerHTML\")        \n",
    "            save_data_to_file(post_data, f'page_{i+1}.html')\n",
    "            print(i)        \n",
    "            teardown(driver)\n",
    "        \n",
    "data_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('post_urls.csv')\n",
    "urls = df[\"url\"].tolist()\n",
    "\n",
    "d={\"url\":urls,\"title\":[],\"user_name\":[],\"user_title\":[],\"user_profile_link\":[],\"number_of_posts_by_user\":[],\"number_of_solutions_by_user\":[],\"number_of_kudos_by_user\":[],\"timestamp\":[],\"post_content\":[],\"solved\":[],\"labels_list\":[],\"number_of_views\":[],\"number_of_likes\":[],\"replies\":[],\"accepted_sol\":[]}\n",
    "\n",
    "for i,file in enumerate(os.listdir(\"data\")):\n",
    "    with open(f\"data/{file}\",encoding=\"utf8\") as f:\n",
    "        html_doc=f.read()\n",
    "    soup=BeautifulSoup(html_doc,\"html.parser\")    \n",
    "    \n",
    "    title=soup.find(\"h1\",class_=\"page-title\")\n",
    "    if title==None:\n",
    "        continue\n",
    "    d[\"title\"].append(title.text)\n",
    "    \n",
    "    user_name=soup.find(\"a\",class_=\"lia-user-name-link\")\n",
    "    d[\"user_name\"].append(user_name.text)\n",
    "    \n",
    "    user_profile_link=user_name.get_attribute_list(\"href\")\n",
    "    d[\"user_profile_link\"].append(user_profile_link[0])\n",
    "   \n",
    "    user_title_container=soup.find(\"span\",class_=\"lia-message-unread\")\n",
    "    user_title=user_title_container.find_next(\"div\")    \n",
    "    d[\"user_title\"].append(user_title.text.strip())\n",
    "    \n",
    "    number_of_posts_by_user=soup.find(class_=\"lia-message-author-posts-count\")\n",
    "    d[\"number_of_posts_by_user\"].append(number_of_posts_by_user.text)\n",
    "    \n",
    "    number_of_solutions_by_user=soup.find(class_=\"lia-message-author-solutions-count\")\n",
    "    d[\"number_of_solutions_by_user\"].append(number_of_solutions_by_user.text.strip())\n",
    "    \n",
    "    nunber_of_kudos_by_user=soup.find(class_=\"lia-message-author-kudos-count\")\n",
    "    d[\"number_of_kudos_by_user\"].append(nunber_of_kudos_by_user.text.strip())\n",
    "    \n",
    "    publish_date=soup.select(\".DateTime > meta\")[0].get_attribute_list(\"content\")\n",
    "    d[\"timestamp\"].append(publish_date[0])\n",
    "    \n",
    "    post_content_data=soup.find(class_=\"lia-message-body-content\")\n",
    "    for content in post_content_data(['style', 'script']):\n",
    "        content.decompose().strip()\n",
    "    post_content=' '.join(post_content_data.stripped_strings)\n",
    "    d[\"post_content\"].append(post_content)    \n",
    "    \n",
    "    if soup.find(class_=\"lia-panel-feedback-banner-safe\"):\n",
    "        solve=\"Y\"\n",
    "        accepted_sol=soup.find_all(class_=\"lia-message-body-content\")\n",
    "        for content in accepted_sol[1](['style', 'script']):\n",
    "                content.decompose().strip()\n",
    "        sol_content=' '.join(accepted_sol[1].stripped_strings)\n",
    "\n",
    "    else:\n",
    "        solve=\"N\"\n",
    "        sol_content=\" \"\n",
    "    d[\"solved\"].append(solve)\n",
    "    d[\"accepted_sol\"].append(sol_content)\n",
    "    \n",
    "    label_data=soup.find_all(class_=\"label\")\n",
    "    labels=[]\n",
    "    for label_arr in label_data:\n",
    "        labels.append((label_arr.text).strip())\n",
    "    d[\"labels_list\"].append(labels)\n",
    "        \n",
    "    view_count=soup.find(class_=\"lia-message-stats-count\")\n",
    "    if view_count:\n",
    "        d[\"number_of_views\"].append(view_count.text.strip())   \n",
    "    else:\n",
    "        d[\"number_of_views\"].append(\"0\")   \n",
    "  \n",
    "    \n",
    "    like_count=soup.find(class_=\"MessageKudosCount\")\n",
    "    d[\"number_of_likes\"].append(like_count.text.strip())  \n",
    "    \n",
    "    replies=soup.find(class_=\"lia-threaded-detail-display-message-view\")\n",
    "    reply_list=[]\n",
    "    if replies:\n",
    "        reply_data=soup.find_all(class_=\"lia-message-body-content\")\n",
    "        for content in reply_data[1](['style', 'script']):\n",
    "            content.decompose().strip()\n",
    "        reply_content=' '.join(reply_data[1].stripped_strings)\n",
    "        reply_list.append(reply_content)\n",
    "    else:\n",
    "        pass\n",
    "    d[\"replies\"].append(reply_list)\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "df=pd.DataFrame(data=d);\n",
    "df.to_csv(\"shopify_discussion_data.csv\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
